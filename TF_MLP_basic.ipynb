{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/urim/Data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/urim/Data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/urim/Data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/urim/Data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/Users/urim/Data/MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape\n",
    "mnist.train.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 \ttr_loss: 413.587280 \ttr_acc: 0.140000\n",
      "\t\tval_loss: 352.827606 \tval_acc: 0.080000\n",
      "step: 100 \ttr_loss: 4.812678 \ttr_acc: 0.590000\n",
      "step: 200 \ttr_loss: 2.196179 \ttr_acc: 0.340000\n",
      "\t\tval_loss: 1.588027 \tval_acc: 0.400000\n",
      "step: 300 \ttr_loss: 2.011850 \ttr_acc: 0.320000\n",
      "step: 400 \ttr_loss: 2.111109 \ttr_acc: 0.400000\n",
      "\t\tval_loss: 1.846839 \tval_acc: 0.350000\n",
      "step: 500 \ttr_loss: 2.037679 \ttr_acc: 0.410000\n",
      "step: 600 \ttr_loss: 1.948010 \ttr_acc: 0.390000\n",
      "\t\tval_loss: 1.815727 \tval_acc: 0.360000\n",
      "step: 700 \ttr_loss: 1.371984 \ttr_acc: 0.450000\n",
      "step: 800 \ttr_loss: 1.707211 \ttr_acc: 0.480000\n",
      "\t\tval_loss: 1.255183 \tval_acc: 0.510000\n",
      "step: 900 \ttr_loss: 1.209413 \ttr_acc: 0.550000\n",
      "step: 1000 \ttr_loss: 1.517931 \ttr_acc: 0.440000\n",
      "\t\tval_loss: 1.209993 \tval_acc: 0.500000\n",
      "\t\tte_loss: 1.472670 \tte_acc: 0.480000\n"
     ]
    }
   ],
   "source": [
    "mnist.train.labels[0]\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "labels = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal(shape=[mnist.train.images.shape[1], 128]))\n",
    "b1 = tf.Variable(tf.random_normal(shape=[128]))\n",
    "layer1 = tf.nn.relu(tf.matmul(inputs, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal(shape=[128, 32]))\n",
    "b2 = tf.Variable(tf.random_normal(shape=[32]))\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal(shape=[32,10]))\n",
    "b3 = tf.Variable(tf.random_normal(shape=[10]))\n",
    "logits = tf.matmul(layer2, W3)+b3\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=0.005).minimize(loss)\n",
    "\n",
    "outputs = tf.nn.softmax(logits)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, axis=1), tf.argmax(labels, axis=1)), tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1001):\n",
    "        batch_tr_inputs, batch_tr_labels = mnist.train.next_batch(100)\n",
    "        _, tr_loss, tr_acc = sess.run([train_step, loss, accuracy], \n",
    "                                      feed_dict={inputs:batch_tr_inputs, labels:batch_tr_labels})\n",
    "\n",
    "        batch_val_inputs, batch_val_labels = mnist.validation.next_batch(100)\n",
    "        val_loss, val_acc = sess.run([loss, accuracy], \n",
    "                                      feed_dict={inputs:batch_val_inputs, labels:batch_val_labels})\n",
    "        \n",
    "        batch_te_inputs, batch_te_labels = mnist.test.next_batch(100)\n",
    "        te_loss, te_acc = sess.run([loss, accuracy], \n",
    "                                      feed_dict={inputs:batch_te_inputs, labels:batch_te_labels})\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('step: {} \\ttr_loss: {:4f} \\ttr_acc: {:4f}'.format(i, tr_loss, tr_acc))\n",
    "        if i % 200 == 0:\n",
    "            print('\\t\\tval_loss: {:4f} \\tval_acc: {:4f}'.format(val_loss, val_acc))\n",
    "        if i == 1000:\n",
    "            print('\\t\\tte_loss: {:4f} \\tte_acc: {:4f}'.format(te_loss, te_acc))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
